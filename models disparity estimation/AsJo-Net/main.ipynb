{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from tensorboardX import SummaryWriter\n",
    "from datasets import __datasets__\n",
    "from models import __models__, model_loss_train, model_loss_test\n",
    "from utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../config.json') as f:\n",
    "    config = json.load(f)\n",
    "# BASE_DIR = config[\"BASE_DIR_WIN\"]\n",
    "# CKPT_DIR = config[\"BASE_DIR_CGI_CKPT_WIN\"]\n",
    "BASE_DIR = config[\"BASE_DIR\"]\n",
    "CKPT_DIR = config[\"BASE_DIR_CGI_CKPT_MAC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    model: str           = 'CGI_Stereo'\n",
    "    maxdisp: int         = 192\n",
    "    dataset: str         = 'scape_pipes'\n",
    "    datapath: str        = Path(BASE_DIR)\n",
    "    trainlist: str       = Path(\"dataset_paths/half_aug_combined_rectified_scape_dataset_train.txt\")\n",
    "    testlist: str        = Path(\"dataset_paths/half_aug_combined_rectified_scape_dataset_test.txt\")\n",
    "    lr: float            = 0.001\n",
    "    batch_size: int      = 1\n",
    "    test_batch_size: int = 1\n",
    "    epochs: int          = 100\n",
    "    lrepochs: str        = \"10,14,16,18:2\"\n",
    "    logdir: str          = os.path.join(CKPT_DIR, \"ScapeCombinedAugmented_2\")\n",
    "    # loadckpt: str        = os.path.join(CKPT_DIR, \"Sceneflow\", \"checkpoint_000098.ckpt\")\n",
    "    loadckpt: str        = False\n",
    "    resume: bool         = False\n",
    "    seed: int            = 42\n",
    "    summary_freq: int    = 1\n",
    "    save_freq: int       = 1\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "os.makedirs(args.logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new summary file\n"
     ]
    }
   ],
   "source": [
    "# create summary logger\n",
    "print(\"creating new summary file\")\n",
    "logger = SummaryWriter(args.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, dataloader\n",
    "StereoDataset = __datasets__[args.dataset]\n",
    "train_dataset = StereoDataset(args.datapath, args.trainlist, True)\n",
    "test_dataset = StereoDataset(args.datapath, args.testlist, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainImgLoader = DataLoader(train_dataset, args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "TestImgLoader = DataLoader(test_dataset, args.test_batch_size, shuffle=False, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer\n",
    "model = __models__[args.model](args.maxdisp)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
    "logger.add_text('args', str(args))\n",
    "logger.add_text('model', str(model))\n",
    "logger.add_text('optimiser', str(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start at epoch 0\n"
     ]
    }
   ],
   "source": [
    "# load parameters\n",
    "start_epoch = 0\n",
    "if args.resume:\n",
    "\n",
    "    # find all checkpoints file and sort according to epoch id\n",
    "    all_saved_ckpts = [fn for fn in os.listdir(args.logdir) if fn.endswith(\".ckpt\")]\n",
    "    all_saved_ckpts = sorted(all_saved_ckpts, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    # use the latest checkpoint file\n",
    "    loadckpt = os.path.join(os.path.abspath(args.logdir), all_saved_ckpts[-1])\n",
    "    print(\"loading the lastest model in logdir: {}\".format(loadckpt))\n",
    "    state_dict = torch.load(loadckpt)\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer'])\n",
    "    start_epoch = state_dict['epoch'] + 1\n",
    "elif args.loadckpt:\n",
    "    # load the checkpoint file specified by args.loadckpt\n",
    "    print(\"loading model {}\".format(args.loadckpt))\n",
    "    state_dict = torch.load(args.loadckpt)\n",
    "    model_dict = model.state_dict()\n",
    "    pre_dict = {k: v for k, v in state_dict['model'].items() if k in model_dict}\n",
    "    model_dict.update(pre_dict) \n",
    "    # model.load_state_dict(state_dict['model'])\n",
    "    model.load_state_dict(model_dict)\n",
    "print(\"start at epoch {}\".format(start_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train one sample\n",
    "def train_sample(sample, compute_metrics=False):\n",
    "    model.train()\n",
    "    imgL, imgR, disp_gt, disp_gt_low = sample['left'], sample['right'], sample['disparity'], sample['disparity_low']\n",
    "\n",
    "    disp_gt = torch.abs(disp_gt)\n",
    "    disp_gt_low = torch.abs(disp_gt_low)\n",
    "\n",
    "    imgL = imgL.to(device)\n",
    "    imgR = imgR.to(device)\n",
    "    disp_gt = disp_gt.to(device)\n",
    "    disp_gt_low = disp_gt_low.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    disp_ests = model(imgL, imgR)\n",
    "    mask = (disp_gt < args.maxdisp) & (disp_gt > 0)\n",
    "    mask_low = (disp_gt_low < args.maxdisp) & (disp_gt_low > 0)\n",
    "    masks = [mask, mask_low]\n",
    "    disp_gts = [disp_gt, disp_gt_low] \n",
    "    loss = model_loss_train(disp_ests, disp_gts, masks)\n",
    "    disp_ests_final = [disp_ests[0]]\n",
    "\n",
    "    scalar_outputs = {\"loss\": loss}\n",
    "    # image_outputs = {\"disp_est\": disp_ests, \"disp_gt\": disp_gt, \"imgL\": imgL, \"imgR\": imgR}\n",
    "    if compute_metrics:\n",
    "        with torch.no_grad():\n",
    "            # image_outputs[\"errormap\"] = [disp_error_image_func()(disp_est, disp_gt) for disp_est in disp_ests_final]\n",
    "            scalar_outputs[\"EPE\"] = [EPE_metric(disp_est, disp_gt, mask) for disp_est in disp_ests_final]\n",
    "            scalar_outputs[\"D1\"] = [D1_metric(disp_est, disp_gt, mask) for disp_est in disp_ests_final]\n",
    "            # scalar_outputs[\"Thres1\"] = [Thres_metric(disp_est, disp_gt, mask, 1.0) for disp_est in disp_ests_final]\n",
    "            # scalar_outputs[\"Thres2\"] = [Thres_metric(disp_est, disp_gt, mask, 2.0) for disp_est in disp_ests_final]\n",
    "            # scalar_outputs[\"Thres3\"] = [Thres_metric(disp_est, disp_gt, mask, 3.0) for disp_est in disp_ests_final]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return tensor2float(loss), tensor2float(scalar_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one sample\n",
    "@make_nograd_func\n",
    "def test_sample(sample, compute_metrics=True):\n",
    "    model.eval()\n",
    "    imgL, imgR, disp_gt = sample['left'], sample['right'], sample['disparity']\n",
    "\n",
    "    disp_gt = torch.abs(disp_gt)\n",
    "\n",
    "    imgL = imgL.to(device)\n",
    "    imgR = imgR.to(device)\n",
    "    disp_gt = disp_gt.to(device)\n",
    "\n",
    "    disp_ests = model(imgL, imgR)\n",
    "\n",
    "    # plt.imshow(disp_gt[0].detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "    # plt.imshow(disp_ests[0][0].detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "\n",
    "    mask = (disp_gt < args.maxdisp) & (disp_gt > 0)\n",
    "    masks = [mask]\n",
    "    disp_gts = [disp_gt]\n",
    "    loss = model_loss_test(disp_ests, disp_gts, masks)\n",
    "\n",
    "    scalar_outputs = {\"loss\": loss}\n",
    "\n",
    "    image_outputs = {\"disp_est\": disp_ests, \"disp_gt\": disp_gt, \"imgL\": imgL, \"imgR\": imgR}\n",
    "\n",
    "    scalar_outputs[\"D1\"] = [D1_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
    "    scalar_outputs[\"EPE\"] = [EPE_metric(disp_est, disp_gt, mask) for disp_est in disp_ests]\n",
    "    scalar_outputs[\"Thres1\"] = [Thres_metric(disp_est, disp_gt, mask, 1.0) for disp_est in disp_ests]\n",
    "    scalar_outputs[\"Thres2\"] = [Thres_metric(disp_est, disp_gt, mask, 2.0) for disp_est in disp_ests]\n",
    "    scalar_outputs[\"Thres3\"] = [Thres_metric(disp_est, disp_gt, mask, 3.0) for disp_est in disp_ests]\n",
    "\n",
    "    if compute_metrics:\n",
    "        # image_outputs[\"errormap\"] = [disp_error_image_func()(disp_est, disp_gt) for disp_est in disp_ests]\n",
    "        image_outputs[\"errormap\"] = [disp_error_image_func.apply(disp_est, disp_gt) for disp_est in disp_ests]\n",
    "\n",
    "    return tensor2float(loss), tensor2float(scalar_outputs), image_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_histogram(errormap):\n",
    "    errormap_flat = errormap.flatten()\n",
    "    plt.figure(figsize=(12, 6))  # Adjust the size as needed\n",
    "    plt.hist(errormap_flat, bins=24, range=(0, 1), align='mid')\n",
    "    plt.xlabel('Normalized Error Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Normalized Error Values')\n",
    "    plt.show()\n",
    "\n",
    "def generate_error_label_counts(errormap):\n",
    "    errormap_flat = errormap.flatten()\n",
    "    labels, counts = np.unique(errormap_flat, return_counts=True)\n",
    "    error_label_counts = dict(zip(labels, counts))\n",
    "    return error_label_counts\n",
    "\n",
    "def local_plot_error_histogram(image_outputs):\n",
    "    image_outputs = tensor2numpy(image_outputs)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5), dpi =300)\n",
    "    ax[0].imshow(image_outputs[\"disp_est\"][0][0])\n",
    "    ax[0].set_title(\"disp_est\")\n",
    "    ax[1].imshow(image_outputs[\"disp_gt\"][0])\n",
    "    ax[1].set_title(\"disp_gt\")\n",
    "    ax[2].imshow(image_outputs[\"imgL\"][0][0], cmap='gray')\n",
    "    ax[2].set_title(\"imgL\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"{args.logdir}/test_{global_step:0>6}.png\")\n",
    "\n",
    "    errormap = image_outputs[\"errormap\"][0][0]\n",
    "    errormap = np.transpose(errormap, (1, 2, 0))  # Transpose to (H, W, C) format\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    plt.imshow(errormap, cmap='RdYlBu_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    bestepoch = 0\n",
    "    error = 100\n",
    "    for epoch_idx in range(start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch_idx, args.lr, args.lrepochs)\n",
    "\n",
    "        # training\n",
    "        avg_train_scalars = AverageMeterDict()\n",
    "        for batch_idx, sample in enumerate(TrainImgLoader):\n",
    "            global_step = len(TrainImgLoader) * epoch_idx + batch_idx\n",
    "            start_time = time.time()\n",
    "            do_summary = True\n",
    "            # do_summary = global_step % args.summary_freq == 0\n",
    "            loss, scalar_outputs = train_sample(sample, compute_metrics=do_summary)\n",
    "            if do_summary:\n",
    "                save_scalars(logger, 'train', scalar_outputs, global_step)\n",
    "                # save_images(logger, 'train', image_outputs, global_step)\n",
    "            avg_train_scalars.update(scalar_outputs)\n",
    "            del scalar_outputs\n",
    "            print(f'Epoch {epoch_idx+1}/{args.epochs}, Iter {batch_idx+1}/{len(TrainImgLoader)}, train loss = {loss:.3f}, time = {time.time() - start_time:.3f}')\n",
    "        avg_train_scalars = avg_train_scalars.mean()\n",
    "        save_scalars(logger, 'train', avg_train_scalars, epoch_idx + 1)\n",
    "\n",
    "        # saving checkpoints\n",
    "        if (epoch_idx + 1) % args.save_freq == 0:\n",
    "            checkpoint_data = {'epoch': epoch_idx, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "            torch.save(checkpoint_data, f\"{args.logdir}/checkpoint_{epoch_idx:0>6}.ckpt\")\n",
    "        gc.collect()\n",
    "\n",
    "        # testing\n",
    "        avg_test_scalars = AverageMeterDict()\n",
    "        bestepoch = 0\n",
    "        error = 100\n",
    "        for batch_idx, sample in enumerate(TestImgLoader):\n",
    "            global_step = (len(TestImgLoader) * epoch_idx + batch_idx) + 1\n",
    "            start_time = time.time()\n",
    "            loss, scalar_outputs, image_outputs = test_sample(sample, compute_metrics=do_summary)\n",
    "\n",
    "            # save image outputs\n",
    "            do_summary = True\n",
    "            # do_summary = global_step % args.summary_freq == 0\n",
    "            if do_summary:\n",
    "                save_scalars(logger, 'test', scalar_outputs, global_step)\n",
    "                # save_images(logger, 'test', image_outputs, epoch_idx +1 )\n",
    "\n",
    "            local_visualise = False\n",
    "            if local_visualise:\n",
    "                errormap = image_outputs[\"errormap\"][0][0]\n",
    "                local_plot_error_histogram(image_outputs)\n",
    "                plot_error_histogram(errormap)\n",
    "                error_label_counts = generate_error_label_counts(errormap)\n",
    "                print(error_label_counts)\n",
    "\n",
    "            avg_test_scalars.update(scalar_outputs)\n",
    "            del scalar_outputs\n",
    "            print(f'Epoch {epoch_idx+1}/{args.epochs}, Iter {batch_idx+1}/{len(TestImgLoader)},  test loss = {loss:.3f}, time = { time.time() - start_time:3f}')\n",
    "\n",
    "            \n",
    "        avg_test_scalars = avg_test_scalars.mean()\n",
    "        nowerror = avg_test_scalars[\"D1\"][0]\n",
    "        if  nowerror < error :\n",
    "            bestepoch = epoch_idx\n",
    "            error = avg_test_scalars[\"D1\"][0]\n",
    "        save_scalars(logger, 'validation', avg_test_scalars, epoch_idx + 1)\n",
    "        print(\"avg_test_scalars\", avg_test_scalars)\n",
    "        print('MAX epoch %d total test error = %.5f' % (bestepoch, error))\n",
    "        gc.collect()\n",
    "    print('MAX epoch %d total test error = %.5f' % (bestepoch, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speciale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
